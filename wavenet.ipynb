{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea is to create a WaveNet\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import nn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(names)=32033\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Prepping Data Set and Data Loader:\n",
    "#Data:\n",
    "names = open(\"names.txt\").read().splitlines()\n",
    "print(f\"{len(names)=}\")\n",
    "\n",
    "# Build Vocab\n",
    "itos = {i+1:s for i,s in enumerate(sorted(set(\"\".join(names))))}\n",
    "itos[0] = '.'\n",
    "\n",
    "num_embeddings = len(itos)\n",
    "print(num_embeddings)\n",
    "stoi = {s:i for i,s in itos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Set Creation:\n",
    "def create_data(names):\n",
    "    x, y = [], []\n",
    "    context_len = 8\n",
    "    xchar, ychar = [], []\n",
    "    buff = [0]*context_len\n",
    "    buff = [itos[b] for b in buff]\n",
    "    for name in names:\n",
    "        augmented_name = buff + list(name) + ['.']\n",
    "        for i in range(len(augmented_name)-context_len):\n",
    "            xi = augmented_name[i:i+context_len ]\n",
    "            yi = augmented_name[i+context_len]\n",
    "            #print(f\"{xi} -------> {yi}\")\n",
    "            xchar.append(xi)\n",
    "            ychar.append(yi)\n",
    "            x.append([stoi[d] for d in xi])\n",
    "            y.append(stoi[yi])\n",
    "    \n",
    "    return torch.tensor(x), torch.tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25626, 3203, 3203)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Minibathces:\n",
    "import random\n",
    "random.shuffle(names)\n",
    "\n",
    "train = int(0.8 * len(names))\n",
    "val = int(0.1 * len(names))\n",
    "test = int(0.1 * len(names))\n",
    "\n",
    "train, test, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Ytr = create_data(names[:train])\n",
    "Xval, Yval = create_data(names[train:train+val])\n",
    "Xtest, Ytest = create_data(names[train+val:train+val+test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ --> a\n",
      ".......a --> l\n",
      "......al --> o\n",
      ".....alo --> n\n",
      "....alon --> a\n",
      "...alona --> h\n",
      "..alonah --> .\n",
      "........ --> k\n",
      ".......k --> e\n",
      "......ke --> i\n",
      ".....kei --> a\n",
      "....keia --> r\n",
      "...keiar --> a\n",
      "..keiara --> .\n",
      "........ --> o\n",
      ".......o --> l\n",
      "......ol --> u\n",
      ".....olu --> w\n",
      "....oluw --> a\n",
      "...oluwa --> t\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(Xtr[:20], Ytr[:20]):\n",
    "  print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76579\n"
     ]
    }
   ],
   "source": [
    "# hierarchical network\n",
    "n_embd = 24 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 128 # the number of neurons in the hidden layer of the MLP\n",
    "model = nn.Sequential([\n",
    "  nn.Embedding(num_embeddings, n_embd),\n",
    "  nn.FlattenConsecutive(2), nn.Linear(n_embd * 2, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "  nn.FlattenConsecutive(2), nn.Linear(n_hidden*2, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "  nn.FlattenConsecutive(2), nn.Linear(n_hidden*2, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "  nn.Linear(n_hidden, num_embeddings),\n",
    "])\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
